{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2e6f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.system('tar -zxvf dataset_roboflow.tar.gz && tar -zxvf dataset_sketch_it.tar.gz')\n",
    "\n",
    "# !tar -zxvf dataset_roboflow.tar.gz && tar -zxvf dataset_sketch_it.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffefabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset_path='./new_dataset/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9668ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import pandas as pd\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "import urllib\n",
    "import os\n",
    "import csv\n",
    "import cv2\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "from keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\n",
    "from keras_retinanet.utils.visualization import draw_box, draw_caption\n",
    "from keras_retinanet.utils.colors import label_color\n",
    "\n",
    "import torch, detectron2\n",
    "\n",
    "# COMMON LIBRARIES\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "from datetime import datetime\n",
    "# from google.colab.patches import cv2_imshow\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot as plt\n",
    "# %matplotlib inline\n",
    "\n",
    "# DATA SET PREPARATION AND LOADING\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data import DatasetCatalog, MetadataCatalog\n",
    "\n",
    "# VISUALIZATION\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.utils.visualizer import ColorMode\n",
    "\n",
    "# CONFIGURATION\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.config import get_cfg\n",
    "\n",
    "# EVALUATION\n",
    "from detectron2.engine import DefaultPredictor\n",
    "\n",
    "# TRAINING\n",
    "from detectron2.engine import DefaultTrainer\n",
    "\n",
    "# from keras_retinanet import models/\n",
    "# from keras_retinanet.utils.image import read_image_bgr, preprocess_image, resize_image\n",
    "# from keras_retinanet.utils.visualization import draw_box, draw_caption\n",
    "# from keras_retinanet.utils.colors import label_color\n",
    "\n",
    "# %matplotlib inline\n",
    "# %config InlineBackend.figure_format='retina'\n",
    "\n",
    "register_matplotlib_converters()\n",
    "\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "np.random.seed(RANDOM_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cbdcea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "THRES_SCORE = 0.4\n",
    "\n",
    "def draw_detections(image, box, scores=0, label=''):\n",
    "#   for box, score, label in zip(boxes[0], scores[0], labels[0]):\n",
    "#     if score < THRES_SCORE:\n",
    "#         continue\n",
    "\n",
    "#     color = label_color(labels)\n",
    "\n",
    "#     box = box.astype(int)\n",
    "    \n",
    "    draw_box(image, box, color=(0,255,0) )\n",
    "\n",
    "    caption = \"{} {:.3f}\".format(label, 0)\n",
    "    draw_caption(image, box, caption)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "def show_detected_objects(image_row,img_folder=''):\n",
    "    \n",
    "    img_path = img_folder+image_row.filename\n",
    "    true_box = [\n",
    "        image_row.xmin, image_row.ymin, image_row.xmax, image_row.ymax]\n",
    "    image = read_image_bgr(img_path)\n",
    "\n",
    "    draw = image.copy()\n",
    "    draw = cv2.cvtColor(draw, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    image = preprocess_image(image)\n",
    "    image, scale = resize_image(image)\n",
    "\n",
    "#     boxes, scores, labels = model.predict_on_batch(np.expand_dims(image, axis = 0))\n",
    "#     print('scale',scale)\n",
    "#     print('scores',scores)\n",
    "#     print('labels',labels)\n",
    "#     print('image',image)\n",
    "\n",
    "#     boxes /= scale\n",
    "#     print('boxes',boxes[0])\n",
    "\n",
    "    draw_box(draw, true_box, color=(255, 0, 0))\n",
    "    draw_detections(image, box=true_box, label= labels_to_name)\n",
    "    \n",
    "#     caption = \"{} {:.3f}\".format(image_row.class_name,0)\n",
    "#     draw_caption(image,true_box , image_row.class_name)\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.imshow(draw)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3691aa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "roboflow = pd.read_csv('dataset_roboflow/new_train/annotations.csv')\n",
    "sketch_it = pd.read_csv('dataset_sketch_it/images/annotations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ccd88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "roboflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322dfb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sketch_it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63c2157",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = roboflow.append(sketch_it,ignore_index=True)\n",
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f877c9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset.rename(columns={'image_name': 'filename'}, inplace=True)\n",
    "new_dataset.rename(columns={'x_min': 'xmin'}, inplace=True)\n",
    "new_dataset.rename(columns={'y_min': 'ymin'}, inplace=True)\n",
    "new_dataset.rename(columns={'x_max': 'xmax'}, inplace=True)\n",
    "new_dataset.rename(columns={'y_max': 'ymax'}, inplace=True)\n",
    "new_dataset.rename(columns={'class_name': 'label'}, inplace=True)\n",
    "\n",
    "\n",
    "roboflow.rename(columns={'image_name': 'filename'}, inplace=True)\n",
    "roboflow.rename(columns={'x_min': 'xmin'}, inplace=True)\n",
    "roboflow.rename(columns={'y_min': 'ymin'}, inplace=True)\n",
    "roboflow.rename(columns={'x_max': 'xmax'}, inplace=True)\n",
    "roboflow.rename(columns={'y_max': 'ymax'}, inplace=True)\n",
    "roboflow.rename(columns={'class_name': 'label'}, inplace=True)\n",
    "\n",
    "sketch_it.rename(columns={'image_name': 'filename'}, inplace=True)\n",
    "sketch_it.rename(columns={'x_min': 'xmin'}, inplace=True)\n",
    "sketch_it.rename(columns={'y_min': 'ymin'}, inplace=True)\n",
    "sketch_it.rename(columns={'x_max': 'xmax'}, inplace=True)\n",
    "sketch_it.rename(columns={'y_max': 'ymax'}, inplace=True)\n",
    "sketch_it.rename(columns={'class_name': 'label'}, inplace=True)\n",
    "\n",
    "new_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd7ab85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import imghdr\n",
    "\n",
    "def copy2dri(df,destination,fromDir=[]):\n",
    "\n",
    "    print(destination)\n",
    "\n",
    "    for i in df.filename: \n",
    "\n",
    "\n",
    "    #     create dir\n",
    "        if not os.path.exists(destination):\n",
    "            os.makedirs(destination)\n",
    "\n",
    "        for Dir in fromDir:\n",
    "#             print(f'{Dir+i} -')\n",
    "            \n",
    "            if os.path.exists(Dir+i):\n",
    "                pli_img = Image.open(Dir+i)  \n",
    "                cv_img = cv2.imread(Dir+i)\n",
    "\n",
    "#                 print(f'{Dir+i} - {pli_img.format}')\n",
    "        #         cv2.imwrite(destination+'/'+i, cv_img, [int(cv2.IMWRITE_JPEG_QUALITY), 100])  \n",
    "                shutil.copy2(Dir+i,destination)\n",
    "\n",
    "\n",
    "    df.to_csv(destination+'/annotations.csv',index=False)      \n",
    "\n",
    "    return 'done'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc588619",
   "metadata": {},
   "source": [
    "# filter by accpted classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dd4cee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels_to_name=pd.DataFrame( new_dataset['label'].unique()).sort_values(by=0)\n",
    "labels_to_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfecf0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_dataset.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6455024b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accpted_classes = pd.read_csv('./accpted_classes.csv')\n",
    "accpted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093ee44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accpted_classes_list = accpted_classes.values.reshape(1,-1)[0]\n",
    "\n",
    "classes = []\n",
    "\n",
    "for i,item in enumerate(accpted_classes_list):\n",
    "        classes.append({'name':item, 'id':i+1})\n",
    "\n",
    "\n",
    "with open('label_map.pbtxt', 'w') as f:\n",
    "    for item in classes:\n",
    "        f.write('item { \\n')\n",
    "        f.write('\\tname:\\'{}\\'\\n'.format(item['name']))\n",
    "        f.write('\\tid:{}\\n'.format(item['id']))\n",
    "        f.write('}\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c69c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = new_dataset[new_dataset.label.isin(list(accpted_classes.values.reshape(1,-1)[0]))]\n",
    "new_dataset\n",
    "# accpted_classes.values.reshape(1,-1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225cb06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "copy2dri(destination=new_dataset_path,df= new_dataset,fromDir=['dataset_roboflow/new_train/','dataset_sketch_it/images/'])\n",
    "\n",
    "# destination='./new_dataset'\n",
    "# 'dataset_roboflow/new_train/'\n",
    "# 'dataset_sketch_it/images/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fb9126",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = pd.read_csv('./new_dataset/annotations.csv')\n",
    "new_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "280e582b",
   "metadata": {},
   "source": [
    "# show all object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff89cf",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# create coco from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60eece38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# path = 'tensorflow2csv.csv'\n",
    "save_json_path = 'new_dataset_coco.json'\n",
    "\n",
    "def csv2coco( data ,save_json_path):\n",
    "\n",
    "#     data = new_dataset.copy()\n",
    "\n",
    "    images = []\n",
    "    categories = []\n",
    "    annotations = []\n",
    "\n",
    "    category = {}\n",
    "    category[\"supercategory\"] = 'none'\n",
    "    category[\"id\"] = 0\n",
    "    category[\"name\"] = 'None'\n",
    "    categories.append(category)\n",
    "\n",
    "    data['fileid'] = data['filename'].astype('category').cat.codes\n",
    "    data['categoryid']= pd.Categorical(data['label'],ordered= True).codes\n",
    "    data['categoryid'] = data['categoryid']+1\n",
    "    data['annid'] = data.index\n",
    "\n",
    "    def image(row):\n",
    "        image = {}\n",
    "        image[\"height\"] = row.height\n",
    "        image[\"width\"] = row.width\n",
    "        image[\"id\"] = row.fileid\n",
    "        image[\"file_name\"] = row.filename\n",
    "        return image\n",
    "\n",
    "    def category(row):\n",
    "        category = {}\n",
    "        category[\"supercategory\"] = 'None'\n",
    "        category[\"id\"] = row.categoryid\n",
    "        category[\"name\"] = row.label\n",
    "        return category\n",
    "\n",
    "    def annotation(row):\n",
    "        annotation = {}\n",
    "        area = (row.xmax -row.xmin)*(row.ymax - row.ymin)\n",
    "        annotation[\"segmentation\"] = []\n",
    "        annotation[\"iscrowd\"] = 0\n",
    "        annotation[\"area\"] = area\n",
    "        annotation[\"image_id\"] = row.fileid\n",
    "\n",
    "        annotation[\"bbox\"] = [row.xmin, row.ymin, row.xmax -row.xmin,row.ymax-row.ymin ]\n",
    "\n",
    "        annotation[\"category_id\"] = row.categoryid\n",
    "        annotation[\"id\"] = row.annid\n",
    "        return annotation\n",
    "\n",
    "    for row in data.itertuples():\n",
    "        annotations.append(annotation(row))\n",
    "\n",
    "    imagedf = data.drop_duplicates(subset=['fileid']).sort_values(by='fileid')\n",
    "    for row in imagedf.itertuples():\n",
    "        images.append(image(row))\n",
    "\n",
    "    catdf = data.drop_duplicates(subset=['categoryid']).sort_values(by='categoryid')\n",
    "    for row in catdf.itertuples():\n",
    "        categories.append(category(row))\n",
    "\n",
    "    data_coco = {}\n",
    "    data_coco[\"images\"] = images\n",
    "    data_coco[\"categories\"] = categories\n",
    "    data_coco[\"annotations\"] = annotations\n",
    "\n",
    "\n",
    "    json.dump(data_coco, open(save_json_path, \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d900f9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv2coco(new_dataset.copy(),'new_dataset/annotations.coco.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a8e988",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# for i in range(3):\n",
    "#     item = new_dataset.iloc[i]\n",
    "#     print(f'{i}- {item.filename} - {item.label}')\n",
    "#     show_detected_objects(item,img_folder='new_dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e057abd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SET_NAME = 'new_dataset'\n",
    "ANNOTATIONS_FILE_NAME = \"annotations.coco.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c60f509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN SET\n",
    "DATA_SET_NAME = f\"{DATA_SET_NAME}\"\n",
    "DATA_SET_IMAGES_DIR_PATH = os.path.join(DATA_SET_NAME)\n",
    "DATA_SET_ANN_FILE_PATH = os.path.join(DATA_SET_NAME, ANNOTATIONS_FILE_NAME)\n",
    "\n",
    "register_coco_instances(\n",
    "    name=DATA_SET_NAME, \n",
    "    metadata={}, \n",
    "    json_file=DATA_SET_ANN_FILE_PATH, \n",
    "    image_root=DATA_SET_IMAGES_DIR_PATH\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08a945b",
   "metadata": {},
   "outputs": [],
   "source": [
    "[\n",
    "    data_set\n",
    "    for data_set\n",
    "    in MetadataCatalog.list()\n",
    "    if data_set.startswith(DATA_SET_NAME)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a1b1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "metadata = MetadataCatalog.get(DATA_SET_NAME)\n",
    "dataset_train = DatasetCatalog.get(DATA_SET_NAME)\n",
    "\n",
    "print(metadata.thing_classes)\n",
    "\n",
    "dataset_entry = dataset_train[random.choice(range(0,len(dataset_train)))]\n",
    "image = cv2.imread(dataset_entry[\"file_name\"])\n",
    "\n",
    "visualizer = Visualizer(\n",
    "    image[:, :, ::-1],\n",
    "    metadata=metadata, \n",
    "    scale=0.8, \n",
    "    instance_mode=ColorMode.IMAGE_BW\n",
    ")\n",
    "\n",
    "out = visualizer.draw_dataset_dict(dataset_entry)\n",
    "# cv2_imshow(out.get_image()[:, :, ::-1])\n",
    "\n",
    "plt.figure(figsize=(15,10))\n",
    "plt.imshow(out.get_image()[:, :, ::-1])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88411e17",
   "metadata": {},
   "source": [
    "# spilt into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da450bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_dataset.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70802296",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "\n",
    "\n",
    "if os.path.exists('train'):\n",
    "    shutil.rmtree('train')\n",
    "    \n",
    "if os.path.exists('test'):\n",
    "    shutil.rmtree('test')\n",
    "\n",
    "train_set = pd.DataFrame([])\n",
    "test_set = pd.DataFrame([])\n",
    "lbls = new_dataset.label.unique().tolist()\n",
    "\n",
    "for lbl in lbls:\n",
    "\n",
    "    train, test = train_test_split(new_dataset[new_dataset['label']==lbl], test_size = 0.2,shuffle=False)\n",
    "    train_set=train_set.append(train,ignore_index=True)\n",
    "    test_set=test_set.append(test,ignore_index=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565c5400",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf973a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d8a3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "copy2dri(destination='train/',df= train_set,fromDir=[new_dataset_path])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b792fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "copy2dri(destination='test/',df= test_set,fromDir=[new_dataset_path])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145323d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set = pd.read_csv('test/annotations.csv')\n",
    "# train_set = pd.read_csv('train/annotations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5179c335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12e6b64c",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# create coco from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9b3d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "# path = 'tensorflow2csv.csv'\n",
    "save_json_path = 'new_dataset_coco.json'\n",
    "\n",
    "def csv2coco( data ,save_json_path):\n",
    "\n",
    "#     data = new_dataset.copy()\n",
    "\n",
    "    images = []\n",
    "    categories = []\n",
    "    annotations = []\n",
    "\n",
    "    category = {}\n",
    "    category[\"supercategory\"] = 'none'\n",
    "    category[\"id\"] = 0\n",
    "    category[\"name\"] = 'None'\n",
    "    categories.append(category)\n",
    "\n",
    "    data['fileid'] = data['filename'].astype('category').cat.codes\n",
    "    data['categoryid']= pd.Categorical(data['label'],ordered= True).codes\n",
    "    data['categoryid'] = data['categoryid']+1\n",
    "    data['annid'] = data.index\n",
    "\n",
    "    def image(row):\n",
    "        image = {}\n",
    "        image[\"height\"] = row.height\n",
    "        image[\"width\"] = row.width\n",
    "        image[\"id\"] = row.fileid\n",
    "        image[\"file_name\"] = row.filename\n",
    "        return image\n",
    "\n",
    "    def category(row):\n",
    "        category = {}\n",
    "        category[\"supercategory\"] = 'None'\n",
    "        category[\"id\"] = row.categoryid\n",
    "        category[\"name\"] = row.label\n",
    "        return category\n",
    "\n",
    "    def annotation(row):\n",
    "        annotation = {}\n",
    "        area = (row.xmax -row.xmin)*(row.ymax - row.ymin)\n",
    "        annotation[\"segmentation\"] = []\n",
    "        annotation[\"iscrowd\"] = 0\n",
    "        annotation[\"area\"] = area\n",
    "        annotation[\"image_id\"] = row.fileid\n",
    "\n",
    "        annotation[\"bbox\"] = [row.xmin, row.ymin, row.xmax -row.xmin,row.ymax-row.ymin ]\n",
    "\n",
    "        annotation[\"category_id\"] = row.categoryid\n",
    "        annotation[\"id\"] = row.annid\n",
    "        return annotation\n",
    "\n",
    "    for row in data.itertuples():\n",
    "        annotations.append(annotation(row))\n",
    "\n",
    "    imagedf = data.drop_duplicates(subset=['fileid']).sort_values(by='fileid')\n",
    "    for row in imagedf.itertuples():\n",
    "        images.append(image(row))\n",
    "\n",
    "    catdf = data.drop_duplicates(subset=['categoryid']).sort_values(by='categoryid')\n",
    "    for row in catdf.itertuples():\n",
    "        categories.append(category(row))\n",
    "\n",
    "    data_coco = {}\n",
    "    data_coco[\"images\"] = images\n",
    "    data_coco[\"categories\"] = categories\n",
    "    data_coco[\"annotations\"] = annotations\n",
    "\n",
    "\n",
    "    json.dump(data_coco, open(save_json_path, \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f30c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv2coco(train_set.copy(),'train/annotations.coco.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bbac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv2coco(test_set.copy(),'test/annotations.coco.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfe2b2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176b27bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aacba24b",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# create tf record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585fbd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# from __future__ import division\n",
    "# from __future__ import print_function\n",
    "# from __future__ import absolute_import\n",
    "\n",
    "import os\n",
    "import io\n",
    "import sys\n",
    "import pandas as pd\n",
    "import tensorflow.compat.v1 as tf\n",
    "# import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "from object_detection.utils import dataset_util, label_map_util\n",
    "from collections import namedtuple, OrderedDict\n",
    "\n",
    "# flags = tf.app.flags\n",
    "# flags.DEFINE_string('csv_input', '', 'Path to the CSV input')\n",
    "# flags.DEFINE_string('output_path', '', 'Path to output TFRecord')\n",
    "# flags.DEFINE_string('image_dir', '', 'Path to images')\n",
    "# FLAGS = flags.FLAGS\n",
    "\n",
    "class_map = label_map_util.load_labelmap('label_map.pbtxt')\n",
    "class_map_dict = label_map_util.get_label_map_dict(class_map)\n",
    "\n",
    "# print(class_map_dict)\n",
    "\n",
    "\n",
    "\n",
    "# TO-DO replace this with label map\n",
    "def class_text_to_int(row_label):\n",
    "    return class_map_dict[row_label]\n",
    "\n",
    "\n",
    "def split(df, group):\n",
    "    data = namedtuple('data', ['filename', 'object'])\n",
    "    gb = df.groupby(group)\n",
    "    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n",
    "\n",
    "\n",
    "def create_tf_example(group, path):\n",
    "    \n",
    "    with tf.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n",
    "        encoded_jpg = fid.read()\n",
    "    encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
    "    image = Image.open(encoded_jpg_io)\n",
    "\n",
    "    width, height = image.size\n",
    "\n",
    "#     print(encoded_jpg)\n",
    "    \n",
    "    filename = group.filename.encode('utf8')\n",
    "#     print(image_name)\n",
    "    image_format = image.format.encode()\n",
    "    xmins = []\n",
    "    xmaxs = []\n",
    "    ymins = []\n",
    "    ymaxs = []\n",
    "    classes_text = []\n",
    "    classes_num = []\n",
    "\n",
    "    for index, row in group.object.iterrows():\n",
    "#         print(row['x_min'])\n",
    "        xmins.append(row['xmin'] / width)\n",
    "        xmaxs.append(row['xmax'] / width)\n",
    "        ymins.append(row['ymin'] / height)\n",
    "        ymaxs.append(row['ymax'] / height)\n",
    "        classes_text.append(row['label'].encode('utf8'))\n",
    "        classes_num.append(class_text_to_int(row['label']))\n",
    "        \n",
    "#     print(image_format)\n",
    "\n",
    "\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature={\n",
    "        'image/height': dataset_util.int64_feature(height),\n",
    "        'image/width': dataset_util.int64_feature(width),\n",
    "        'image/filename': dataset_util.bytes_feature(filename),\n",
    "        'image/source_id': dataset_util.bytes_feature(filename),\n",
    "        'image/encoded': dataset_util.bytes_feature(encoded_jpg),\n",
    "        'image/format': dataset_util.bytes_feature(image_format),\n",
    "        'image/object/bbox/xmin': dataset_util.float_list_feature(xmins),\n",
    "        'image/object/bbox/xmax': dataset_util.float_list_feature(xmaxs),\n",
    "        'image/object/bbox/ymin': dataset_util.float_list_feature(ymins),\n",
    "        'image/object/bbox/ymax': dataset_util.float_list_feature(ymaxs),\n",
    "        'image/object/class/text': dataset_util.bytes_list_feature(classes_text),\n",
    "        'image/object/class/label': dataset_util.int64_list_feature(classes_num),\n",
    "    }))\n",
    "        \n",
    "    return tf_example\n",
    "\n",
    "\n",
    "def csv_2_tfrecord(output_path,image_dir,csv_input):\n",
    "    \n",
    "    writer = tf.python_io.TFRecordWriter(output_path)\n",
    "    path = os.path.join(image_dir)\n",
    "    examples = pd.read_csv(csv_input)\n",
    "    grouped = split(examples, 'filename')\n",
    "#     print(grouped)\n",
    "    \n",
    "\n",
    "    # added\n",
    "    file_errors = 0\n",
    "\n",
    "    for group in grouped:\n",
    "        try:\n",
    "            tf_example = create_tf_example(group, path)            \n",
    "            writer.write(tf_example.SerializeToString())\n",
    "        except:\n",
    "\n",
    "            # added\n",
    "            file_errors += 1\n",
    "            pass\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "    # added\n",
    "    print(\"FINISHED. There were %d errors\" % file_errors)\n",
    "\n",
    "    output_path = os.path.join(os.getcwd(), output_path)\n",
    "    print('Successfully created the TFRecords: {}'.format(output_path))\n",
    "\n",
    "    \n",
    "# call\n",
    "csv_2_tfrecord(csv_input='train/annotations.csv',\n",
    "              image_dir='train/',\n",
    "              output_path= 'train.record')\n",
    "\n",
    "csv_2_tfrecord(csv_input='test/annotations.csv',\n",
    "              image_dir='test/',\n",
    "              output_path= 'test.record')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c24b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a94f739",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
